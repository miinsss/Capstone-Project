{"cells":[{"cell_type":"markdown","metadata":{"id":"ul8FG0MadDKn"},"source":["# Neural Network"]},{"cell_type":"markdown","metadata":{"id":"D3ZcHUY1On1h"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Install required libraries\n","!pip install dask[dataframe] dask-ml tensorflow\n","\n","import dask.dataframe as dd\n","from dask_ml.preprocessing import DummyEncoder, StandardScaler\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from dask_ml.model_selection import train_test_split\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1717012793577,"user":{"displayName":"Mina Randolf","userId":"17262056063041429157"},"user_tz":-120},"id":"XgxCpF7I74U6"},"outputs":[],"source":["import pandas as pd\n","# from google.colab import drive"]},{"cell_type":"markdown","metadata":{"id":"G9UsEZkYznAS"},"source":["## Load the data"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":146882,"status":"ok","timestamp":1717011809117,"user":{"displayName":"Mina Randolf","userId":"17262056063041429157"},"user_tz":-120},"id":"yVqv0p9NMhBy","outputId":"17e4fafc-c6f4-405d-fdd1-4818e55fe3e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["\"\"\"\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Define the file path and dtype\n","file_path = '/content/drive/My Drive/Capstone/Data/contract_classification.csv'\n","dtype = {\n","    'CORPORATE_DEVISION': 'category',\n","    'ORTS-NAME': 'category',\n","    'STRASSE': 'category',\n","    'CONSTRACTION_DESIGN': 'category',\n","    'ZONE': 'category',\n","    'PRODUCTLINE': 'category',\n","    'UNDERWRITER': 'category',\n","    'PARTY-ID': 'category'\n","}\n","\n","# Load data in chunks\n","df = dd.read_csv(file_path, dtype=dtype, blocksize=\"64MB\")\n","\n","# Ensure that the categories are known\n","categorical_columns = list(dtype.keys())\n","df = df.categorize(columns=categorical_columns)\n","\"\"\""]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65200,"status":"ok","timestamp":1717012858773,"user":{"displayName":"Mina Randolf","userId":"17262056063041429157"},"user_tz":-120},"id":"pR3-Ub4EOz_E","outputId":"ffe85685-fa6e-4073-89f1-1637b7f9ddd9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-6-8cd155ac6077>:6: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(file_path)\n"]},{"name":"stdout","output_type":"stream","text":["Number of observations: 11574439\n","Column names:\n","Index(['ANO_SID', 'CORPORATE_DEVISION', 'ORTPLZ', 'ORTS-NAME', 'STRASSE',\n","       'SUM_INSURED', 'CONSTRACTION_DESIGN', 'CONSTRUCTION_YEAR', 'WFL',\n","       'ZONE', 'SF-SYSTEM', 'TYPE_OF_DEDUCTIBLE', 'DRAIN_PIPE_INSURED',\n","       'PRODUCTLINE', 'PRIOR_DAMAGES', 'UVV-KZ', 'UNDERWRITER', 'PARTY-ID',\n","       'contract_year', 'PIPE_PREMIUM_AMOUNT', 'YEAR', 'DAMAGE'],\n","      dtype='object')\n","Column types:\n","ANO_SID                float64\n","CORPORATE_DEVISION      object\n","ORTPLZ                 float64\n","ORTS-NAME               object\n","STRASSE                 object\n","SUM_INSURED            float64\n","CONSTRACTION_DESIGN     object\n","CONSTRUCTION_YEAR      float64\n","WFL                    float64\n","ZONE                    object\n","SF-SYSTEM              float64\n","TYPE_OF_DEDUCTIBLE       int64\n","DRAIN_PIPE_INSURED       int64\n","PRODUCTLINE             object\n","PRIOR_DAMAGES            int64\n","UVV-KZ                   int64\n","UNDERWRITER             object\n","PARTY-ID                object\n","contract_year           object\n","PIPE_PREMIUM_AMOUNT    float64\n","YEAR                     int64\n","DAMAGE                   int64\n","dtype: object\n"]}],"source":["# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Load the CSV file\n","file_path = '/content/drive/My Drive/Capstone/Data/contract_classification.csv'\n","df = pd.read_csv(file_path)\n","\n","# Count all observations\n","num_observations = len(df)\n","print(f\"Number of observations: {num_observations}\")\n","\n","# List all column names\n","column_names = df.columns\n","print(\"Column names:\")\n","print(column_names)\n","\n","# Display the types of the columns\n","column_types = df.dtypes\n","print(\"Column types:\")\n","print(column_types)\n"]},{"cell_type":"markdown","metadata":{"id":"-G08M52oQSV3"},"source":["## Preprocess the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PEzx7Qu3Pb-g"},"outputs":[],"source":["\"\"\"\n","# Convert categorical columns to dummy variables\n","df = DummyEncoder(columns=categorical_columns).fit_transform(df)\n","\n","# Handle missing values\n","df = df.fillna(0)\n","\n","# Reduce memory usage by converting float64 to float32\n","float_columns = df.select_dtypes(include=['float64']).columns\n","df[float_columns] = df[float_columns].astype('float32')\n","\n","# Convert to Dask arrays for compatibility with TensorFlow\n","df = df.to_dask_array(lengths=True)\n","\"\"\""]},{"cell_type":"code","execution_count":3,"metadata":{"id":"F8qH3sKnVqNp"},"outputs":[{"name":"stderr","output_type":"stream","text":["<>:7: SyntaxWarning: invalid escape sequence '\\.'\n","<>:7: SyntaxWarning: invalid escape sequence '\\.'\n","C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_5800\\558881398.py:7: SyntaxWarning: invalid escape sequence '\\.'\n","  file_path = '..\\..\\data\\contract_classification.csv'\n","C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_5800\\558881398.py:7: SyntaxWarning: invalid escape sequence '\\.'\n","  file_path = '..\\..\\data\\contract_classification.csv'\n"]},{"ename":"MemoryError","evalue":"Unable to allocate 81.1 GiB for an array with shape (391900, 222182) and data type bool","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[1;32mIn[3], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Standardize the features\u001b[39;00m\n\u001b[0;32m     34\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 35\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Compute the Dask DataFrame to get a sample to fit the scaler\u001b[39;00m\n\u001b[0;32m     38\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m X_scaled\u001b[38;5;241m.\u001b[39mcompute()\n","File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n","File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n","File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask_ml\\preprocessing\\data.py:85\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     82\u001b[0m     attributes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvar_\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m var_\n\u001b[0;32m     84\u001b[0m attributes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 85\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mattributes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(attributes, values):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, k, v)\n","File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n","File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask_expr\\_expr.py:3758\u001b[0m, in \u001b[0;36mFused._execute_task\u001b[1;34m(graph, name, *deps)\u001b[0m\n\u001b[0;32m   3756\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, dep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(deps):\n\u001b[0;32m   3757\u001b[0m     graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m dep\n\u001b[1;32m-> 3758\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask_expr\\_dummies.py:177\u001b[0m, in \u001b[0;36mGetDummies.operation\u001b[1;34m(df, *args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moperation\u001b[39m(df, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_meta_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\encoding.py:353\u001b[0m, in \u001b[0;36m_get_dummies_1d\u001b[1;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     dummy_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_\n\u001b[1;32m--> 353\u001b[0m dummy_mat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdummy_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m dummy_mat[np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(codes)), codes] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dummy_na:\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# reset NaN GH4446\u001b[39;00m\n","\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 81.1 GiB for an array with shape (391900, 222182) and data type bool"]}],"source":["import dask.dataframe as dd\n","from dask_ml.preprocessing import DummyEncoder, StandardScaler\n","from dask_ml.model_selection import train_test_split\n","\n","# Load the CSV file with dask and specify the dtypes for the categorical columns\n","file_path = '/content/drive/My Drive/Capstone/Data/contract_classification.csv'\n","\n","dtype = {\n","    'CORPORATE_DEVISION': 'category',\n","    'ORTS-NAME': 'category',\n","    'STRASSE': 'category',\n","    'CONSTRACTION_DESIGN': 'category',\n","    'ZONE': 'category',\n","    'PRODUCTLINE': 'category',\n","    'UNDERWRITER': 'category',\n","    'PARTY-ID': 'category'\n","}\n","df = dd.read_csv(file_path, dtype=dtype)\n","\n","# Ensure that the categories are known\n","df = df.categorize(columns=dtype.keys())\n","\n","# Convert categorical columns to dummy variables\n","df = DummyEncoder(columns=dtype.keys()).fit_transform(df)\n","\n","# Handle missing values\n","df = df.dropna()\n","\n","# Split the data into features and target\n","X = df.drop(columns=['DAMAGE'])\n","y = df['DAMAGE']\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Compute the Dask DataFrame to get a sample to fit the scaler\n","X_scaled = X_scaled.compute()\n","y = y.compute()\n","\n","# Example: Split for the years 2014 (train) and 2015 (test)\n","def rolling_window_train_test_split(X, y, train_year, test_year):\n","    train_index = df[df['YEAR'] == train_year].index\n","    test_index = df[df['YEAR'] == test_year].index\n","    X_train, X_test = X_scaled.loc[train_index], X_scaled.loc[test_index]\n","    y_train, y_test = y.loc[train_index], y.loc[test_index]\n","    return X_train, X_test, y_train, y_test\n","\n","train_year = 2014\n","test_year = 2015\n","X_train, X_test, y_train, y_test = rolling_window_train_test_split(X, y, train_year, test_year)\n"]},{"cell_type":"markdown","metadata":{"id":"kwxXODW2YInA"},"source":["## Modelling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NlkJuSRE5QoX"},"outputs":[],"source":["\"\"\"\n","years = df['contract_year'].unique().compute()\n","years.sort()\n","\n","recalls = []\n","\n","for i in range(len(years) - 1):\n","    train_year = years[i]\n","    test_year = years[i + 1]\n","\n","    # Split the data based on the year\n","    train_data = df[df['contract_year'] == train_year]\n","    test_data = df[df['contract_year'] == test_year]\n","\n","    # Split the data into features and target\n","    X_train = train_data.drop('DAMAGE', axis=1)\n","    y_train = train_data['DAMAGE']\n","    X_test = test_data.drop('DAMAGE', axis=1)\n","    y_test = test_data['DAMAGE']\n","\n","    # Convert to Dask arrays for compatibility with TensorFlow\n","    X_train = X_train.to_dask_array(lengths=True)\n","    X_test = X_test.to_dask_array(lengths=True)\n","    y_train = y_train.to_dask_array(lengths=True)\n","    y_test = y_test.to_dask_array(lengths=True)\n","\n","    # Build the neural network\n","    model = Sequential([\n","        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n","        Dense(32, activation='relu'),\n","        Dense(1, activation='sigmoid')\n","    ])\n","\n","    # Compile the model\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['Recall'])\n","\n","    # Train the model\n","    model.fit(X_train, y_train, epochs=10, batch_size=256, validation_data=(X_test, y_test))\n","\n","    # Evaluate the model\n","    _, recall = model.evaluate(X_test, y_test)\n","    recalls.append(recall)\n","\n","    print(f'Trained on {train_year}, tested on {test_year}, Recall: {recall}')\n","\n","# Print the average recall\n","average_recall = np.mean(recalls)\n","print(f'Average Recall: {average_recall}')\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjuqV_8yYJDn"},"outputs":[],"source":["# Define and Train the Neural Network Model\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","# Define the neural network model\n","model = Sequential()\n","model.add(Dense(32, activation='relu', input_dim=X_train.shape[1]))\n","model.add(Dense(16, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['Recall'])\n","\n","# Print the model summary\n","model.summary()\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"]},{"cell_type":"markdown","metadata":{"id":"RLDNuc-G5p5H"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U5lBDe3m5rRm"},"outputs":[],"source":["\"\"\"\n","# Evaluate the overall performance\n","average_recall = np.mean(recalls)\n","print(f'Average Recall over all years: {average_recall}')\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J-J_p-sHYOVo"},"outputs":[],"source":["# Evaluate the model\n","loss, recall = model.evaluate(X_test, y_test)\n","print(f'Test Recall: {recall}')\n","\n","# Plot training & validation recall values\n","import matplotlib.pyplot as plt\n","\n","plt.plot(history.history['recall'])\n","plt.plot(history.history['val_recall'])\n","plt.title('Model recall')\n","plt.ylabel('Recall')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMmMzAS4WpZmv4I8SMs6BeT","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
