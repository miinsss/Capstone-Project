{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c6436ac-cfa8-4fe2-a51a-fb4e3a2514a0",
   "metadata": {},
   "source": [
    "# Balance the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4a217-d62d-4874-9033-d0caf6e95cd0",
   "metadata": {},
   "source": [
    "## Installations and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f61f9a-616d-4e66-b0f4-4a602c275ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: dask[complete]\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "!pip install dask[complete] dask-ml imbalanced-learn\n",
    "\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65256bbf-a39a-48f8-a861-5cb5e5a1bb10",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497b81c1-4869-4d15-8b5e-3a82da1d1277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/minarandolf/Capstone/Capstone-Project/datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/72/ct286z0d1tz0s971c6b7qshh0000gn/T/ipykernel_1877/947416810.py:12: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations: 9466361\n",
      "Column names:\n",
      "Index(['Unnamed: 0', 'ANO_SID', 'CORPORATE_DEVISION', 'Bundesland', 'Typ',\n",
      "       'ORTPLZ', 'CONSTRACTION_DESIGN', 'CONSTRUCTION_YEAR', 'WFL', 'ZONE',\n",
      "       'TYPE_OF_DEDUCTIBLE', 'DRAIN_PIPE_INSURED', 'PRODUCTLINE',\n",
      "       'PRIOR_DAMAGES', 'UVV-KZ', 'UNDERWRITER', 'YEAR',\n",
      "       'DAMAGE_HEAVY_RAIN_ZONE', 'LONGITUDE', 'LATITUDE', 'DAMAGE'],\n",
      "      dtype='object')\n",
      "Column types:\n",
      "Unnamed: 0                  int64\n",
      "ANO_SID                   float64\n",
      "CORPORATE_DEVISION         object\n",
      "Bundesland                 object\n",
      "Typ                        object\n",
      "ORTPLZ                      int64\n",
      "CONSTRACTION_DESIGN        object\n",
      "CONSTRUCTION_YEAR         float64\n",
      "WFL                       float64\n",
      "ZONE                       object\n",
      "TYPE_OF_DEDUCTIBLE          int64\n",
      "DRAIN_PIPE_INSURED          int64\n",
      "PRODUCTLINE                object\n",
      "PRIOR_DAMAGES               int64\n",
      "UVV-KZ                      int64\n",
      "UNDERWRITER                object\n",
      "YEAR                        int64\n",
      "DAMAGE_HEAVY_RAIN_ZONE    float64\n",
      "LONGITUDE                 float64\n",
      "LATITUDE                  float64\n",
      "DAMAGE                      int64\n",
      "dtype: object\n",
      "   Unnamed: 0    ANO_SID CORPORATE_DEVISION           Bundesland    Typ  \\\n",
      "0           0  4114028.0                VHV  Nordrhein-Westfalen  Stadt   \n",
      "1           1  4114039.0                VHV  Nordrhein-Westfalen  Stadt   \n",
      "2           2  4114045.0                VHV  Nordrhein-Westfalen  Stadt   \n",
      "3           3  4114049.0                VGV  Nordrhein-Westfalen  Stadt   \n",
      "4           4  4114055.0                VHV  Nordrhein-Westfalen  Kreis   \n",
      "\n",
      "   ORTPLZ CONSTRACTION_DESIGN  CONSTRUCTION_YEAR        WFL ZONE  ...  \\\n",
      "0   42109      NORMAL_VENTURE        1967.565648   69.00000  2.0  ...   \n",
      "1   42277      NORMAL_VENTURE        1967.565648   65.00000  4.0  ...   \n",
      "2   42389      DESIGN_CLASS_I        1967.565648   75.00000  1.0  ...   \n",
      "3   42277      NORMAL_VENTURE        1967.565648  106.24368  NaN  ...   \n",
      "4   42553      NORMAL_VENTURE        1967.565648  119.00000  2.0  ...   \n",
      "\n",
      "   DRAIN_PIPE_INSURED  PRODUCTLINE PRIOR_DAMAGES  UVV-KZ  UNDERWRITER  YEAR  \\\n",
      "0                   0        Sonst             0       1            Y  2014   \n",
      "1                   0        Basis             0       1            Y  2014   \n",
      "2                   0        Sonst             0       1            Y  2014   \n",
      "3                   0      Kompakt             0       1            Y  2014   \n",
      "4                   0        Sonst             0       1            Y  2014   \n",
      "\n",
      "   DAMAGE_HEAVY_RAIN_ZONE  LONGITUDE   LATITUDE  DAMAGE  \n",
      "0                1.727273   7.168175  51.285086       0  \n",
      "1                1.931034   7.221409  51.282877       0  \n",
      "2                1.962963   7.257295  51.272306       0  \n",
      "3                1.931034   7.226671  51.285622       0  \n",
      "4                1.768000   7.080255  51.312158       0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Data Loading\n",
    "# Check the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Define the path to your CSV file\n",
    "file_path = '/Users/minarandolf/Capstone/Capstone-Project/datasets/data_all_unique_values.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Number of observations: {len(df)}\")\n",
    "    print(\"Column names:\")\n",
    "    print(df.columns)\n",
    "    print(\"Column types:\")\n",
    "    print(df.dtypes)\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {file_path} does not exist in the current working directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d93142-43de-47ed-9a5e-7c459da7a8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations for each class in 'DAMAGE':\n",
      "DAMAGE\n",
      "0    9330675\n",
      "1     135686\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage distribution of each class in 'DAMAGE':\n",
      "DAMAGE\n",
      "0    98.566651\n",
      "1     1.433349\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Data Analysis\n",
    "# Count the occurrences of each class in the target variable 'DAMAGE'\n",
    "damage_counts = df['DAMAGE'].value_counts()\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of observations for each class in 'DAMAGE':\")\n",
    "print(damage_counts)\n",
    "\n",
    "# Display the percentage distribution\n",
    "damage_percentage = df['DAMAGE'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPercentage distribution of each class in 'DAMAGE':\")\n",
    "print(damage_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e055ad8a-c711-4c0a-9cd5-6c0fc4073d40",
   "metadata": {},
   "source": [
    "## Creating a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea52c390-9603-4e4e-ac81-ee038d70cc07",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'contract_year'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'contract_year'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a smaller subset of the dataset with balanced years and similar distribution of 'DAMAGE'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m subset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20000\u001b[39m\n\u001b[0;32m----> 3\u001b[0m years \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontract_year\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m      4\u001b[0m subset_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m years:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'contract_year'"
     ]
    }
   ],
   "source": [
    "# Create a smaller subset of the dataset with balanced years and similar distribution of 'DAMAGE'\n",
    "subset_size = 20000\n",
    "years = df['contract_year'].unique()\n",
    "subset_list = []\n",
    "\n",
    "for year in years:\n",
    "    year_data = df[df['contract_year'] == year]\n",
    "    no_damage = year_data[year_data['DAMAGE'] == 0]\n",
    "    damage = year_data[year_data['DAMAGE'] == 1]\n",
    "    \n",
    "    # Calculate the sample size for each year based on the total subset size\n",
    "    sample_size = subset_size // len(years)\n",
    "    \n",
    "    # Ensure the sample size is proportionate to the overall distribution\n",
    "    no_damage_sample = no_damage.sample(n=int(sample_size * 0.985), random_state=42, replace=True)\n",
    "    damage_sample = damage.sample(n=int(sample_size * 0.015), random_state=42, replace=True)\n",
    "    \n",
    "    subset_list.append(pd.concat([no_damage_sample, damage_sample]))\n",
    "\n",
    "subset_df = pd.concat(subset_list).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ddf9a5-1408-4ece-98a8-8711518286ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the amount of observations in the subset and the percentage distribution of each class in 'DAMAGE'\n",
    "print(f\"Number of observations in the subset: {len(subset_df)}\")\n",
    "\n",
    "subset_damage_counts = subset_df['DAMAGE'].value_counts()\n",
    "print(\"Number of observations for each class in 'DAMAGE' (subset):\")\n",
    "print(subset_damage_counts)\n",
    "\n",
    "subset_damage_percentage = subset_df['DAMAGE'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPercentage distribution of each class in 'DAMAGE' (subset):\")\n",
    "print(subset_damage_percentage)\n",
    "\n",
    "# Print the number of observations for each year in the subset\n",
    "print(\"\\nNumber of observations for each year in the subset:\")\n",
    "print(subset_df['contract_year'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a361ff-e881-44f0-a592-3c8c7615bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the subset before SMOTE to a CSV file\n",
    "subset_df.to_csv('/Users/minarandolf/Capstone/Capstone-Project/datasets/subset_before_smote.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d806a9-ad01-415f-a1cf-7a1ffce3814e",
   "metadata": {},
   "source": [
    "## Balance the Data using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51c432-554a-4f7e-9746-ba4f74c4daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to load data in chunks and apply SMOTE incrementally\n",
    "def load_and_balance_data(df, chunk_size=5000):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    balanced_data_X = []\n",
    "    balanced_data_y = []\n",
    "\n",
    "    # Process data in chunks\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        end = start + chunk_size\n",
    "        chunk = df.iloc[start:end]\n",
    "        \n",
    "        # Separate features and target\n",
    "        X_chunk = chunk.drop('DAMAGE', axis=1)\n",
    "        y_chunk = chunk['DAMAGE']\n",
    "        \n",
    "        # Ensure numeric columns are in correct format\n",
    "        X_chunk = X_chunk.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        \n",
    "        # Apply SMOTE to the chunk\n",
    "        X_resampled_chunk, y_resampled_chunk = smote.fit_resample(X_chunk, y_chunk)\n",
    "        \n",
    "        balanced_data_X.append(X_resampled_chunk)\n",
    "        balanced_data_y.append(y_resampled_chunk)\n",
    "    \n",
    "    # Concatenate all the resampled chunks\n",
    "    X_resampled = np.vstack(balanced_data_X)\n",
    "    y_resampled = np.hstack(balanced_data_y)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Load and balance the subset data\n",
    "X_resampled, y_resampled = load_and_balance_data(subset_df)\n",
    "\n",
    "# Combine the balanced data into a DataFrame\n",
    "balanced_df = pd.DataFrame(X_resampled, columns=subset_df.drop('DAMAGE', axis=1).columns)\n",
    "balanced_df['DAMAGE'] = y_resampled\n",
    "\n",
    "# Save the balanced subset to a CSV file\n",
    "balanced_df.to_csv('/Users/minarandolf/Capstone/Capstone-Project/datasets/subset_after_smote.csv', index=False)\n",
    "\n",
    "# Check the distribution after SMOTE\n",
    "print(\"Distribution after SMOTE:\")\n",
    "print(pd.Series(y_resampled).value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
